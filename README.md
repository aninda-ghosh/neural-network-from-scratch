# Neural Network From Scratch

## Description

This is a basic neural framework to train a Deep Neural Network with Fully Connected Layers with an arbitrary input and output size. 

## Capabilities and Caveats

### Pros

- Users can choose between two activation functions. Other activation functions (e.g Leaky Relu, Gelu) can be easily integrated in the future.
    - Sigmoid
    - Relu
- Loss Function provided. Other loss functions (e.g Mean Absolute Error, Cross Entropy Error) can be easily integrated in the future.
    - Mean Square Error

### Cons

- Don't have support for Autograd in this version.
- Gradient accumulation is limited in nature.

## Installation



## Usage